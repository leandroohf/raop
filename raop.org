

* Introduction

  Eu vou basear bem no Rpub
  http://www.runzemc.com/2014/08/random-acts-of-pizza.html
  https://rpubs.com/kuhnrl30/RAOP

  * Explain waht is RAoP
  * Discuss a little about article reference
  * Discuss what we decided to do n why
    
* Data

  * Explain what I did in descriptive

    We select the variable in the target data frame below for the
    exploratory analysis
    
  ================================= Importante

  Explicar pq converti para Decile e pq nao considerei valores negativos
  E o motivo foi que para narrative score, muitas narrativas nao pontua gerando 
  uma concentracao no em zero e assim gera discontinidade na distribuicao dos scores
  Ex: 1000 posts com 0 decile e depois 7 decile com 100, 8 com 200 3 assim eu tenho
  a dist do restante dos potnos. 


  ## So zero freq in narrative score are represented as 0 decile
  ## Also affect negative sentiment score but It seems 

    
  #+BEGIN_SRC R :session :tangle raop.R :results none
    ##* ****************************************************************
    ##  Programmer[s]: Leandro Fernandes
    ##  Company/Institution:
    ##  email: leandroohf@gmail.com
    ##  Date: June 20, 2016
    ##  
    ##  The author believes that share code and knowledge is awesome.
    ##  Feel free to share and modify this piece of code. But don't be
    ##  impolite and remember to cite the author and give him his credits.
    ##* ****************************************************************

    library(jsonlite, quietly = TRUE )
    library(feather, quietly = TRUE )
    library(ggplot2, quietly = TRUE )
    library(grid)
    library(gridExtra)
    library(ggthemes)

    source("./utils/data.R")
    source("./utils/utils.R")

    settings   <- fromJSON( "SETTINGS.json", flatten=TRUE)

    cat('Loading data engineer...\n')
    raop.engineer <- readRDS(settings$data_engineer_path)

    data.target <- raop.engineer$GetDataTarget()
  #+END_SRC

* Exploratory

  plot graphs that I found on Rpubs
  
  #+BEGIN_SRC R :session :tangle exp.R
    data.view <- data.target %>%
        dplyr::group_by( requester_upvotes_minus_downvotes_at_request ) %>%
        dplyr::summarise(  success_rate = sum( requester_received_pizza )/ n(),
                         account_age = mean(requester_account_age_in_days_at_request))

    names(data.view)[1] <- c("karma")
    p.karma <- ggplot(data.view, aes(x=karma,y=success_rate)) + geom_line()

    ## =================================
    ## account age
    p.account.age <- ggplot(data.view, aes(x=account_age,y=success_rate)) + geom_line()

    ## =================================
    ## community age
    data.view <- data.target %>%
        dplyr::group_by( community_age ) %>%
        dplyr::summarise( success_rate = sum( requester_received_pizza )/ n())

    names(data.view)[1] <- c("community_age")
    p.community.age <- ggplot(data.view, aes(x=community_age,y=success_rate)) + geom_line()

    grid.arrange(p.karma, p.account.age, p.community.age, ncol = 3)

  #+END_SRC
  
  *Um pto que fiz diferente* Entao eu devo colocar separado.
  
  #+BEGIN_SRC R :session :tangle exp.R
    ## =================================
    ## post sent
    data.view <- data.target %>%
        na.omit() %>%
        dplyr::group_by( post.sent) %>%
        dplyr::summarise( success_rate = sum( requester_received_pizza )/ n())

    names(data.view)[1] <- c("post_sent")
    p <- ggplot(data.view, aes(x=post_sent,y=success_rate)) + geom_line()
    p

    ## =================================
    ## narrative score
    cols.narratives <- c("desire.score",
                         "family.score",
                         "job.score",
                         "money.score",
                         "student.score")

    data.view0 <- data.target %>%
        dplyr::select( dplyr::one_of( c(cols.narratives,
                                        "requester_received_pizza")))

    data.view <- data.frame(decile=integer(),
                            success_rate=double(),
                            narrative=character())

    for ( narrative in cols.narratives){
    
        narrative.view <- data.view0 %>%
            na.omit() %>%
            dplyr::group_by_( narrative ) %>%
            dplyr::summarise( success_rate = sum( requester_received_pizza )/ n())  %>%
            dplyr::mutate(narrative)
    
        names(narrative.view)[1] <- "decile" 
    
        data.view <- rbind(data.view, narrative.view)
    }

    p <- ggplot(data.view, aes(x = decile, y = success_rate, colour = narrative, group = narrative)) +
        geom_line() + ggtitle('Success rate vs. narrative') +
        scale_x_continuous(breaks = seq(0, 10, 1), name = 'Narrative declie') +
        scale_y_continuous(name = 'Success rate')

    p

  #+END_SRC

  #+BEGIN_SRC R :session :tangle exp.R
    ## =================================
    ## first half of the month 
    data.view <- data.target %>%
        na.omit() %>%
        dplyr::group_by( first.half.of.month ) %>%
        dplyr::summarise( success_rate = sum( requester_received_pizza )/ n())

    names(data.view)[1] <- c("first_half")
    p <- ggplot(data.view, aes(x=first_half,y=success_rate)) + geom_bar(stat = 'identity')

    summary(aov(  requester_received_pizza ~ first.half.of.month, data.target ))

    ## =================================
    ## is weekend
    data.view <- data.target %>%
        na.omit() %>%
        dplyr::group_by( is.weekend ) %>%
        dplyr::summarise( success_rate = sum( requester_received_pizza )/ n())

    names(data.view)[1] <- c("is_weekend")
    p <- ggplot(data.view, aes(x=is_weekend,y=success_rate)) + geom_bar(stat = 'identity')

    ## =================================
    ## month
    data.view <- data.target %>%
        na.omit() %>%
        dplyr::group_by( lubridate::month(request.date)) %>%
        dplyr::summarise( success_rate = sum( requester_received_pizza )/ n())

    names(data.view)[1] <- c("month")
    p <- ggplot(data.view, aes(x=month,y=success_rate)) + geom_bar(stat = 'identity')

    ## =================================
    ## month day
    data.view <- data.target %>%
        na.omit() %>%
        dplyr::group_by( lubridate::mday(request.date)) %>%
        dplyr::summarise( success_rate = sum( requester_received_pizza )/ n())

    names(data.view)[1] <- c("month_day")
    p <- ggplot(data.view, aes(x=month_day,y=success_rate)) + geom_bar(stat = 'identity')

    ## =================================
    ## week day
    data.view <- data.target %>%
        na.omit() %>%
        dplyr::group_by( lubridate::wday(request.date)) %>%
        dplyr::summarise( success_rate = sum( requester_received_pizza )/ n())

    names(data.view)[1] <- c("week_day")
    p <- ggplot(data.view, aes(x=week_day,y=success_rate)) + geom_bar(stat = 'identity')
    print(p)

    ## =================================
    ## has posted before
    data.view <- data.target %>%
        na.omit() %>%
        dplyr::group_by( posted.raop.before) %>%
        dplyr::summarise( success_rate = sum( requester_received_pizza )/ n())

    names(data.view)[1] <- c("posted_before")
    p <- ggplot(data.view, aes(x=posted_before,y=success_rate)) + geom_bar(stat = 'identity') 
    print(p)

    summary(aov(requester_received_pizza ~ posted.raop.before , data.target)) 

    ## =================================
    ## has link
    data.view <- data.target %>%
        na.omit() %>%
        dplyr::group_by( has.link) %>%
        dplyr::summarise( success_rate = sum( requester_received_pizza )/ n())

    names(data.view)[1] <- c("has_link")
    p <- ggplot(data.view, aes(x=has_link,y=success_rate)) + geom_bar(stat = 'identity') 
    print(p)

    summary(aov(requester_received_pizza ~ has.link , data.target)) 

    ## =================================
    ## gratitude
    data.view <- data.target %>%
        na.omit() %>%
        dplyr::group_by( gratitude) %>%
        dplyr::summarise( success_rate = sum( requester_received_pizza )/ n())

    names(data.view)[1] <- c("gratitude")
    p <- ggplot(data.view, aes(x=gratitude,y=success_rate)) + geom_bar(stat = 'identity') 
    print(p)

    summary(aov(requester_received_pizza ~ gratitude , data.target)) 

    ## =================================
    ## reciprocity
    data.view <- data.target %>%
        na.omit() %>%
        dplyr::group_by( reciprocity) %>%
        dplyr::summarise( success_rate = sum( requester_received_pizza )/ n())

    names(data.view)[1] <- c("reciprocity")
    p <- ggplot(data.view, aes(x=reciprocity,y=success_rate)) + geom_bar(stat = 'identity') 
    print(p)

    summary(aov(requester_received_pizza ~ reciprocity , data.target)) 

    ## =================================
    ## nword

    p <- ggplot(data.target, aes(x=requester_received_pizza,y=nword)) +
        geom_boxplot()
    print(p)

    p <- ggplot(data.target, aes(x=nword,)) +
        geom_histogram(binwidth = 1) + facet_grid(requester_received_pizza ~ . )
    print(p)

    summary(aov(requester_received_pizza ~ reciprocity , data.target)) 

    data.view <- data.target %>%
        na.omit() %>%
        dplyr::group_by( nword) %>%
        dplyr::summarise( success_rate = sum( requester_received_pizza )/ n())

    names(data.view)[1] <- c("nword")
    p <- ggplot(data.view, aes(x=nword,y=success_rate)) + geom_line()
    print(p)    
  #+END_SRC

* Model
** Development
   Add model creation code and some comments

   glm: 5 min; 6 cores: 1. second
   gbm: 30 min; 6 cores: 1.7 min
   rf: 30 min; 6 cores: 6.3 min; 7 cores: 6.0 min
   nnet: 4 min; 6 cores: 0.87 min

   O ganho com 7 nao eh muito mas a maquina ainda fica utilizavel
   enquanto eu rodo os experimentos

*** Data design (train, test n ensenble dataset)

    #+begin_src R
      library(caret)
      ##library(plyr)
      library(ggplot2)
      library(gridExtra)
      library(pROC)
      library(tictoc)

      library(doMC)
      registerDoMC(cores = 7)

      library(feather, quietly = TRUE )
      library(jsonlite, quietly = TRUE )
      source("./utils/data.R")
      source("./utils/utils.R")
      source("./utils/report.R")
      source("./utils/model.R")

      settings   <- fromJSON( "SETTINGS.json", flatten=TRUE)

      cat('Loading data engineer...\n')
      raop.engineer <- readRDS(settings$data_engineer_path)

      data.target <- raop.engineer$GetDataTarget()


      ## ----------------------------------- [ Data processing ]
      data.target <- data.target[, c("requester_upvotes_minus_downvotes_at_request",
                                 "nword",
                                 "requester_account_age_in_days_at_request",
                                 "requester_days_since_first_post_on_raop_at_request",
                                 "requester_number_of_posts_at_request",
                                 "requester_number_of_posts_on_raop_at_request",
                                 "money.score",
                                 "desire.score",
                                 "family.score",
                                 "job.score",
                                 "student.score",
                                 "post.sent",
                                 "has.link",
                                 "gratitude",
                                 "reciprocity",
                                 "is.weekend",
                                 "community_age",
                                 "first.half.of.month",
                                 "posted.raop.before",                           
                                 "requester_received_pizza")]

      data.target$has.link <- as.numeric(data.target$has.link)
      data.target$first.half.of.month <- as.numeric(data.target$first.half.of.month)
      data.target$posted.raop.before <- as.numeric(data.target$posted.raop.before)
      data.target$gratitude <- as.numeric(data.target$gratitude)
      data.target$reciprocity <- as.numeric(data.target$reciprocity)
      data.target$is.weekend <- as.numeric(data.target$is.weekend)

      data.target$requester_received_pizza <- as.factor(data.target$requester_received_pizza)
      levels(data.target$requester_received_pizza) <- list("fail" = FALSE, "success" = TRUE)

      set.seed(2014)
      train_ind = createDataPartition(data.target$requester_received_pizza,
                                     p = .80, list = FALSE)

      train_te = data.target[-train_ind, ]    # 20% test

      train_m = data.target[train_ind, ]  # 80% for models

      set.seed(2014)
      train_ind = createDataPartition(train_m$requester_received_pizza,
                                     p = .50, list = FALSE)

      train_tr = train_m[train_ind,]  ## 50% to train each model
      train_es = train_m[-train_ind,] ## 50% to train ensemble model 

      ## train the training set

      labelName    <- "requester_received_pizza"  
      ind_vars <- names(train_tr)[names(train_tr) != labelName ]

      cat(dim(train_te))
      cat(dim(train_tr))
      cat(dim(train_es))

      ctrl = trainControl(method = 'cv', summaryFunction = twoClassSummary, classProbs = T)
    #+end_src

*** DONE Logistic Regredssion

    Meu melhor modelo e aparentemente melhor do que o blog que esu
    estava seguindo e com bem menos variaveis com AUC maios

    AUC: 0.675 vs 0.669 (paper) vs 0.664 (blog)

    #+begin_src R

      ## ------------------------------- [ Logistic Regression ]
      set.seed(2014)
      tic("Training: GLM .....")
      logit_m <- train(requester_received_pizza ~
                          requester_upvotes_minus_downvotes_at_request +
                          community_age +
                          nword + has.link + post.sent + posted.raop.before +
                          reciprocity + 
                          family.score + student.score,
                      data = train_tr,
                      method = 'glm', metric = 'ROC', trControl = ctrl)

      toc()

      params <- list()
      params <- CreateRAoPModelDefaultsParams(params,train_data = train_tr,
                                              "Logistic regression tunned mannually based on exploratory phase.")

      params$auc_val <- logit_m$results$ROC
      params$features <- c("requester_upvotes_minus_downvotes_at_request,community_age,
      nword,has.link, post.sent, posted.raop.before, reciprocity,family.score,student.scor")

      auc_es <- GetRAoPAUC(logit_m,train_es[,ind_vars],train_es$requester_received_pizza)
      cat("auc_train_es:", auc_es)

      ## logit_m <- readRDS("models/2017-01-25-lhof-logit_tunned.rds")
      SaveRAoPModel(logit_m,"logit_tunned",params,settings)

   #+end_src

*** DONE GBM

    +*TODO* (Unica coisa q pode explicar o rsultado anterior. senao der desiste)+
    Vou adicionar a dummy>money.score: money.score > 4 => 1
    
    +*TODO*  Se o anterior funcionar tentar+ Nao vou tentar
    desire.money < 3 => 1
    desire.money >=3 and < 8 => 2
    desire.money >= 8 => 3
    
    *Por incrivel que pareca piorou a situacao*
    
    Como eu melhore este resultado? Preciso fazer os graficos de tunning do caret

    http://topepo.github.io/caret/model-training-and-tuning.html
    
    pergunto se eu fizer scale n center (standardize vars) vou ter um melhor resultado?
    http://machinelearningmastery.com/pre-process-your-dataset-in-r/


    Sem preprocessing o melhor que eu consegui foi

    shrinkage: 0.005; ntree: 1500; depth: 4; auc: 0.75; time: ~2.0 minutos
    [[file:scratch/gbm_shr0.005_ntree1500_depth4_auc0.75.png]]

    ---------------------------------
    Nao consegui reproduzir resultado anterior e aprentemente nehuma
    transformacs de cente e scale ajudou a melhorar o resultado

    shrinkage: 0.005; ntree: 800; depth: 7; auc: 0.693; time: ~1.0 minutos
    [[file:scratch/gbm_shr0.005_ntree800_depth7_auc0.693.png]]


   #+begin_src R

     ## ----------------------------------------------- [ GBM ]
     gbm_tune = expand.grid( interaction.depth = c( 4,5, 6,7,8),
                            n.trees = c(400,500, 600, 700, 800, 900, 1000, 1100,1200, 1500, 2000),
                            shrinkage = c(.005),
                            n.minobsinnode = 9)

     set.seed(2014)
     tic("Training: GBM .....")
     gbm_m = train(x = train_tr[, ind_vars], y = train_tr$requester_received_pizza,
                   method = 'gbm', tuneGrid = gbm_tune,
                   metric = 'ROC', verbose = F, trControl = ctrl)

     toc()

     params <- list()
     params <- CreateRAoPModelDefaultsParams(params,train_data = train_tr,
                                             "GBM tunned")

     params$auc_val   <- max(gbm_m$results$ROC)
     params$ntree     <- gbm_m$finalModel$n.trees
     params$depth     <- gbm_m$finalModel$interaction.depth
     params$shrinkage <- gbm_m$finalModel$n.trees


     ## gbm_m <- readRDS("models/2017-01-25-lhof-gbm_tunned.rds")
     auc_es <- GetRAoPAUC(gbm_m,train_es[,ind_vars],train_es$requester_received_pizza)
     cat("auc_train_es:", auc_es)


     SaveRAoPModel(gbm_m,"gbm_tunned",params,settings)

   #+end_src


   #+begin_src R

     preprocess_params <- preProcess(train_tr, method=c("center", "scale"))
     train_tr_pre <- predict(preprocess_params, train_tr )

     ## Restoring defalts boolen variables value
     train_tr_pre[,"has.link"]                  <-  train_tr[,"has.link"]
     train_tr_pre[,"gratitude"]                 <-  train_tr[,"gratitude"]
     train_tr_pre[,"reciprocity"]               <-  train_tr[,"reciprocity"]
     train_tr_pre[,"is.weekend"]                <-  train_tr[,"is.weekend"]
     train_tr_pre[,"first.half.of.month"]       <-  train_tr[,"first.half.of.month"]
     train_tr_pre[,"posted.raop.before"]        <-  train_tr[,"posted.raop.before"]
     train_tr_pre[,"requester_received_pizza" ] <-  train_tr[,"requester_received_pizza" ]

     ## ----------------------------------------------- [ GBM ]
     gbm_tune = expand.grid( interaction.depth = c(2, 3, 4),
                            n.trees = c(500, 1000, 1500, 2000, 2500, 3000),
                            shrinkage = c(.001, .005),
                            n.minobsinnode = 9)

     set.seed(2014)
     tic("Training: GBM .....")
     gbm_m = train(x = train_tr_pre[, ind_vars], y = train_tr_pre$requester_received_pizza,
                   method = 'gbm', tuneGrid = gbm_tune,
                   metric = 'ROC', verbose = F, trControl = ctrl)

     #gbm_imp = varImp(gbm_m)
     toc()

   #+end_src

*** DONE Random Forest
    
    | mtry | ntree |    AUC | comm                                         |
    |------+-------+--------+----------------------------------------------|
    |    3 |   300 |  0.675 |                                              |
    |    2 |  1000 |  0.676 |                                              |
    |    2 |  1500 | 0.6759 |                                              |
    |    2 |  1750 | 0.6761 |                                              |
    |    2 |  2000 |  0.678 | best                                         |
    |    2 |  2250 |  0.675 |                                              |
    |    2 |  2500 |  0.676 |                                              |
    |    3 |  3000 |  0.676 | pouco melhor do q 2; 3000 no mesmo graficoxs |
    |      |  4500 |        |                                              |
    |------+-------+--------+----------------------------------------------|

    =================================
    [2017-01-22 Sun]
    dim(train_r):  4085   21
    
    mtry: 3; ntree: 2250; auc: 0.680
    
    =================================

   #+begin_src R
     ## ------------------------------------- [ Random Forest ]
     #random forests (mtry = 2, roc = .656)
     rf_tune = expand.grid(.mtry =  seq(2,5,1))

     set.seed(2014)
     tic("Training: RF .....")
     rf_m = train(x = train_tr[, ind_vars], y = train_tr$requester_received_pizza,
                  method = 'rf', ntree = 2250, metric = 'ROC', 
                  tuneGrid = rf_tune, trControl = ctrl, importance = T)

     toc()


     params <- list()
     params <- CreateRAoPModelDefaultsParams(params,train_data = train_tr,
                                             "RF tunned")

     params$auc_val   <- max(rf_m$results$ROC)
     params$ntree     <-  rf_m$finalModel$ntree
     params$mtry     <-  rf_m$finalModel$mtry

     ## rf_m <- readRDS("models/2017-01-26-lhof-rf_tunned.rds")
     auc_es <- GetRAoPAUC(rf_m,train_es[,ind_vars],train_es$requester_received_pizza)
     cat("auc_train_es:", auc_es)

     SaveRAoPModel(rf_m,"rf_tunned",params,settings)

   #+end_src
   
*** DONE Neura Network

    Eh provavel que nnet seja o mais sensivel ao sample size
    
    [2017-01-22 Sun]
    dim(train_r):  4085   21
    decay3; size4; auc0.679
    
    [[file:scratch/2017-01-22-nnet-decay3-size4-auc0.693.png]]
    
    =================================

   #+begin_src R
     ## ------------------------------------ [ Neural Network ]
     #nnet (size = 4, decay = 2, roc = .669)
     nnet_tune = expand.grid(size = seq(1,10,1), decay = c(2,3,4))

     set.seed(2014)
     tic("Training: Neural Net .....")
     nnet_m = train(x = train_tr[, ind_vars], y = train_tr$requester_received_pizza,
                    method = 'nnet',maxit = 1000, tuneGrid = nnet_tune,
                    metric = 'ROC', preProc = c('center', 'scale'),
                    verbose = F,
                    trControl = ctrl)

     toc()

     params <- list()
     params <- CreateRAoPModelDefaultsParams(params,train_data = train_tr,
                                                  "nnet tunned")

     params$auc_val   <- max(nnet_m$results$ROC)
     params$decay     <- nnet_m$finalModel$tuneValue$decay
     params$size     <-  nnet_m$finalModel$tuneValue$size

     ## nnet_m <- readRDS("models/2017-01-26-lhof-nnet_tunned.rds")
     auc_es <- GetRAoPAUC(nnet_m,train_es[,ind_vars],train_es$requester_received_pizza)
     cat("auc_train_es:", auc_es)

     SaveRAoPModel(nnet_m,"nnet_tunned",params,settings)
   #+end_src

*** DONE SVM

   [2017-01-22 Sun]
    dim(train_r):  4085   21


    Peerformnace muita baixa comparadao aos outros modelos.
    Acho que nao vale apena tunar

   #+begin_src R
      ## ----------------------------------------------- [ SVM ]

      # Use the expand.grid to specify the search space	
      svm_tune  <- expand.grid(sigma = c(0.001, 0.01, .05),
                               C = c(0.5, 0.75, 0.9, 1, 1.1, 1.25))

      set.seed(2014)
      tic("Training: SVM .....")
      svm_m <- train( x = train_tr[ , ind_vars], y = train_tr$requester_received_pizza,
                     method = "svmRadial", tuneGrid = svm_tune,
                     metric = "ROC", preProc = c("center", "scale"),
                     verbose= F,
                     trControl = ctrl)


      svm_imp = varImp(svm_m)
      svm_imp$model <- "SVM"
      toc()
   #+end_src

*** Simple Model

   #+begin_src R
     ## --------------------------------- [ best simple model ]

     set.seed(2014)
     tic("Training: Simple model .....")
     simple_m <- train(requester_received_pizza ~ nword,
                           data = train_tr,
                       method = 'glm', metric = 'ROC', trControl = ctrl)


     toc()

     params <- list()
     params <- CreateRAoPModelDefaultsParams(params,train_data = train_tr,
                                             "Simple model Logistic regression using nword only.")

     params$auc_val <- simple_m$results$ROC
     params$features <- "nword"

     auc_es <- GetRAoPAUC(simple_m,train_es[,ind_vars],train_es$requester_received_pizza)
     cat("auc_train_es:", auc_es)

     ## simple_m <- readRDS("models/2017-01-26-lhof-simple_tunned.rds")
     SaveRAoPModel(simple_m,"simple_tunned",params,settings)

     simple_p <- predict(simple_m, list(nword = train_te$nword),type="response")
   #+end_src
   
*** Ensemble

   #+begin_src R

     ## --------------------------------- [ Ensemble Baggging ]
     ## combine results

     logit_p <- predict(logit_m, train_te[, ind_vars], type = 'prob')
     rf_p    <- predict(rf_m, train_te[, ind_vars], type = 'prob')
     gbm_p   <- predict(gbm_m, train_te[, ind_vars], type = 'prob')
     nnet_p  <- predict(nnet_m, train_te[, ind_vars], type = 'prob')
     #svm_p   <- predict(svm_m, train_te[, ind_vars], type = 'prob')   

     mean_p <- (logit_p$success + rf_p$success + gbm_p$success + nnet_p$success) / 4
     median_p <- apply(cbind(logit_p$success,rf_p$success,gbm_p$success,nnet_p$success),1,median)

     ## ----------------------------------- [ Ensemble Stack  ]

     train_te$logit_prob <-  predict(logit_m, train_te[, ind_vars], type = 'prob')$success
     train_te$rf_prob    <-  predict(rf_m, train_te[, ind_vars], type = 'prob')$success
     train_te$gbm_prob   <-  predict(gbm_m, train_te[, ind_vars],type ='prob')$success
     train_te$nnet_prob  <-  predict(nnet_m, train_te[, ind_vars], type = 'prob')$success
     #train_te$svm_prob   <-  predict(svm_m, train_te[, ind_vars], type = 'prob')

     train_es$logit_prob <- predict(logit_m, train_es[, ind_vars], type = 'prob')$success
     train_es$rf_prob    <- predict(rf_m, train_es[, ind_vars], type = 'prob')$success
     train_es$gbm_prob   <- predict(gbm_m, train_es[, ind_vars],type ='prob')$success
     train_es$nnet_prob  <- predict(nnet_m, train_es[, ind_vars], type = 'prob')$success
     #train_es$svm_prob   <- predict(svm_m, train_es[, ind_vars], type = 'prob')


     stack_tune = expand.grid( interaction.depth = seq(1, 5, 2),
                            n.trees = seq(500, 2000, 500),
                            shrinkage = c(.01, .1),
                            n.minobsinnode = 9)

     set.seed(2014)
     tic("Training: Ensemble GBM ....")

     stack_vars <- names(train_es)[names(train_es) != labelName ]
     stack_m  <- train(x = train_es[, stack_vars], y = train_es$requester_received_pizza,
                       method = 'gbm', tuneGrid = stack_tune,
                       metric = 'ROC', verbose = F, trControl = ctrl)


     stack_p   <- predict(stack_m, train_te[, stack_vars], type = 'prob')

   #+end_src
   
*** Comparing Models

    Mudar para proxima secao depois

   #+begin_src R
     ## ------------------------------- [ Plot Var Importance ]
     PlotRAoPImportance(logit_m, rf_m, gbm_m, nnet_m)

     ## ---------------------------------- [ Comparing models ]
     ## compare models algorithms (legal esta metodo)
     resamps <- resamples(list(LOGIT = logit_m,
                               GBM = gbm_m,
                               RF  = rf_m,
                               NNET = nnet_m,
                               SIMPLE = simple_m))

     summary(resamps)

     bwplot(resamps, layout = c(1, 3))

     ## --------------------------------------- [ Correlations n  ]

     GetRAopConfusionMatrix(logit_m, train_es[,ind_vars], train_es$requester_received_pizza)
     GetRAopConfusionMatrix(gbm_m, train_es[,ind_vars], train_es$requester_received_pizza)
     GetRAopConfusionMatrix(rf_m, train_es[,ind_vars], train_es$requester_received_pizza)
     GetRAopConfusionMatrix(nnet_m, train_es[,ind_vars], train_es$requester_received_pizza)

     GetRAoPModelsCorrelation(logit_m, gbm_m, rf_m, nnet_m)

    #+end_src
  
** Diagnostics
  
   Como adicionar alguma metrica mais simples como por exemplo
   precision or

   #+begin_src R
     ## compare models algorithms (legal esta metodo)
     resamps <- resamples(list(LOGIT = logit_m,
                               GBM = gbm_m,
                               RF  = rf_m,
                               NNET = nnet_m,
                               SIMPLE = simple_m))

     summary(resamps)

     bwplot(resamps, layout = c(1, 3))
   #+end_src


   =================================

   Permutation test 

* Conclusion
